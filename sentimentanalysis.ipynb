{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lBGz2kYIcx-kfj1jKNazQ60cY2whqJnh",
      "authorship_tag": "ABX9TyNMBgFg3K+Oziqbh5nbtQwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik2627/Machine-Learning/blob/main/sentimentanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZgBHyr11y_Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ],
      "metadata": {
        "id": "LcOrBkCh5YVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input data\n",
        "input_df = pd.read_excel(\"/content/drive/MyDrive/Input.xlsx\")"
      ],
      "metadata": {
        "id": "66TlLy8h7-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract article text from URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            # Assuming article title is within <title> tag and article text is within <p> tags\n",
        "            article_title = soup.title.text.strip()\n",
        "            article_text = \"\\n\".join([p.get_text() for p in soup.find_all('p')])\n",
        "            return article_title, article_text\n",
        "        else:\n",
        "            print(f\"Failed to fetch URL: {url}. Status code: {response.status_code}\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching URL: {url}. Error: {e}\")\n",
        "        return None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "29Nmn2aa8a1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to store extracted articles\n",
        "if not os.path.exists('extracted_articles'):\n",
        "    os.makedirs('extracted_articles')\n"
      ],
      "metadata": {
        "id": "uMORLiTL8kHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over URLs, extract text, and save as text files\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    article_title, article_text = extract_article_text(url)\n",
        "    if article_title and article_text:\n",
        "        # Save article text as a text file\n",
        "        file_name = f\"extracted_articles/{url_id}.txt\"\n",
        "        with open(file_name, 'w', encoding='utf-8') as file:\n",
        "            file.write(f\"Title: {article_title}\\n\\n\")\n",
        "            file.write(article_text)\n",
        "        print(f\"Article text saved for URL_ID: {url_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdG2-ad48oE8",
        "outputId": "0d112b31-ded8-4345-b325-0035920d4cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article text saved for URL_ID: blackassign0001\n",
            "Article text saved for URL_ID: blackassign0002\n",
            "Article text saved for URL_ID: blackassign0003\n",
            "Article text saved for URL_ID: blackassign0004\n",
            "Article text saved for URL_ID: blackassign0005\n",
            "Article text saved for URL_ID: blackassign0006\n",
            "Article text saved for URL_ID: blackassign0007\n",
            "Article text saved for URL_ID: blackassign0008\n",
            "Article text saved for URL_ID: blackassign0009\n",
            "Article text saved for URL_ID: blackassign0010\n",
            "Article text saved for URL_ID: blackassign0011\n",
            "Article text saved for URL_ID: blackassign0012\n",
            "Article text saved for URL_ID: blackassign0013\n",
            "Article text saved for URL_ID: blackassign0014\n",
            "Article text saved for URL_ID: blackassign0015\n",
            "Article text saved for URL_ID: blackassign0016\n",
            "Article text saved for URL_ID: blackassign0017\n",
            "Article text saved for URL_ID: blackassign0018\n",
            "Article text saved for URL_ID: blackassign0019\n",
            "Article text saved for URL_ID: blackassign0020\n",
            "Article text saved for URL_ID: blackassign0021\n",
            "Article text saved for URL_ID: blackassign0022\n",
            "Article text saved for URL_ID: blackassign0023\n",
            "Article text saved for URL_ID: blackassign0024\n",
            "Article text saved for URL_ID: blackassign0025\n",
            "Article text saved for URL_ID: blackassign0026\n",
            "Article text saved for URL_ID: blackassign0027\n",
            "Article text saved for URL_ID: blackassign0028\n",
            "Article text saved for URL_ID: blackassign0029\n",
            "Article text saved for URL_ID: blackassign0030\n",
            "Article text saved for URL_ID: blackassign0031\n",
            "Article text saved for URL_ID: blackassign0032\n",
            "Article text saved for URL_ID: blackassign0033\n",
            "Article text saved for URL_ID: blackassign0034\n",
            "Article text saved for URL_ID: blackassign0035\n",
            "Failed to fetch URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/. Status code: 404\n",
            "Article text saved for URL_ID: blackassign0037\n",
            "Article text saved for URL_ID: blackassign0038\n",
            "Article text saved for URL_ID: blackassign0039\n",
            "Article text saved for URL_ID: blackassign0040\n",
            "Article text saved for URL_ID: blackassign0041\n",
            "Article text saved for URL_ID: blackassign0042\n",
            "Article text saved for URL_ID: blackassign0043\n",
            "Article text saved for URL_ID: blackassign0044\n",
            "Article text saved for URL_ID: blackassign0045\n",
            "Article text saved for URL_ID: blackassign0046\n",
            "Article text saved for URL_ID: blackassign0047\n",
            "Article text saved for URL_ID: blackassign0048\n",
            "Failed to fetch URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/. Status code: 404\n",
            "Article text saved for URL_ID: blackassign0050\n",
            "Article text saved for URL_ID: blackassign0051\n",
            "Article text saved for URL_ID: blackassign0052\n",
            "Article text saved for URL_ID: blackassign0053\n",
            "Article text saved for URL_ID: blackassign0054\n",
            "Article text saved for URL_ID: blackassign0055\n",
            "Article text saved for URL_ID: blackassign0056\n",
            "Article text saved for URL_ID: blackassign0057\n",
            "Article text saved for URL_ID: blackassign0058\n",
            "Article text saved for URL_ID: blackassign0059\n",
            "Article text saved for URL_ID: blackassign0060\n",
            "Article text saved for URL_ID: blackassign0061\n",
            "Article text saved for URL_ID: blackassign0062\n",
            "Article text saved for URL_ID: blackassign0063\n",
            "Article text saved for URL_ID: blackassign0064\n",
            "Article text saved for URL_ID: blackassign0065\n",
            "Article text saved for URL_ID: blackassign0066\n",
            "Article text saved for URL_ID: blackassign0067\n",
            "Article text saved for URL_ID: blackassign0068\n",
            "Article text saved for URL_ID: blackassign0069\n",
            "Article text saved for URL_ID: blackassign0070\n",
            "Article text saved for URL_ID: blackassign0071\n",
            "Article text saved for URL_ID: blackassign0072\n",
            "Article text saved for URL_ID: blackassign0073\n",
            "Article text saved for URL_ID: blackassign0074\n",
            "Article text saved for URL_ID: blackassign0075\n",
            "Article text saved for URL_ID: blackassign0076\n",
            "Article text saved for URL_ID: blackassign0077\n",
            "Article text saved for URL_ID: blackassign0078\n",
            "Article text saved for URL_ID: blackassign0079\n",
            "Article text saved for URL_ID: blackassign0080\n",
            "Article text saved for URL_ID: blackassign0081\n",
            "Article text saved for URL_ID: blackassign0082\n",
            "Article text saved for URL_ID: blackassign0083\n",
            "Article text saved for URL_ID: blackassign0084\n",
            "Article text saved for URL_ID: blackassign0085\n",
            "Article text saved for URL_ID: blackassign0086\n",
            "Article text saved for URL_ID: blackassign0087\n",
            "Article text saved for URL_ID: blackassign0088\n",
            "Article text saved for URL_ID: blackassign0089\n",
            "Article text saved for URL_ID: blackassign0090\n",
            "Article text saved for URL_ID: blackassign0091\n",
            "Article text saved for URL_ID: blackassign0092\n",
            "Article text saved for URL_ID: blackassign0093\n",
            "Article text saved for URL_ID: blackassign0094\n",
            "Article text saved for URL_ID: blackassign0095\n",
            "Article text saved for URL_ID: blackassign0096\n",
            "Article text saved for URL_ID: blackassign0097\n",
            "Article text saved for URL_ID: blackassign0098\n",
            "Article text saved for URL_ID: blackassign0099\n",
            "Article text saved for URL_ID: blackassign0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtAYys0m9W1A",
        "outputId": "1a2422fb-af83-4561-bd23-4a36ce7596e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import textstat\n",
        "import nltk"
      ],
      "metadata": {
        "id": "8_zrn3ao8otX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahH2Lcxk9NeP",
        "outputId": "c40b5b39-7899-4fa6-86ac-9559cbb9fce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_texts = []\n",
        "with open(\"extracted_articles/blackassign0001.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    extracted_texts.append(file.read())"
      ],
      "metadata": {
        "id": "yZeLJeTK9k6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_text_analysis(text):\n",
        "    # Tokenize text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    # Tokenize text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Calculate polarity and subjectivity scores\n",
        "    blob = TextBlob(text)\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
        "\n",
        "    # Calculate percentage of complex words\n",
        "    complex_word_count = sum(1 for word in words if textstat.syllable_count(word) > 2)\n",
        "    percentage_complex_words = complex_word_count / len(words)\n",
        "\n",
        "    # Calculate FOG Index\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate average number of words per sentence\n",
        "    avg_words_per_sentence = len(words) / len(sentences)\n",
        "\n",
        "    # Other calculations as per requirements\n",
        "\n",
        "    return polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence\n",
        "\n",
        "# Perform textual analysis for each extracted text\n",
        "results = []\n",
        "for text in extracted_texts:\n",
        "    polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence = perform_text_analysis(text)\n",
        "    results.append([polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence])\n",
        "\n"
      ],
      "metadata": {
        "id": "tTgUGlR29pUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame(results, columns=['POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE'])\n",
        "output_df.to_csv('output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "i0BsQl_e9wr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}